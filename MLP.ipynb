{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing needed tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import tanh, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class of the network definition and initializing a setting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set eta for from the formula\n",
    "eta=0.3\n",
    "# We have decided to program a perceptron as a class datatype to avoid passing data for every function and initialization convinience\n",
    "class Perceptron:\n",
    "    def __init__(self, layers_structure, transition_funcs, learning_rate, random_seed):\n",
    "        # We control randomness of each run by setting the seed\n",
    "        random.seed(random_seed)\n",
    "        self.num_layers = len(layers_structure)\n",
    "        self.num_X, self.num_y = layers_structure[0], layers_structure[-1]\n",
    "\n",
    "        # a neuron with (2, 3, 4) structure will have two 2x3 and 3x4 matricies as its weights\n",
    "        # we initialize the weights and biases with a random number between -2 and 2\n",
    "        self.weights = [[[(random.random()-0.5)*4 for i in range(layers_structure[h+1])]\n",
    "                            for j in range(layers_structure[h])]\n",
    "                                        for h in range(self.num_layers-1)]\n",
    "\n",
    "        self.biases = [[[(random.random()-0.5)*4 for i in range(layers_structure[h+1])]] for h in range(self.num_layers-1)]\n",
    "\n",
    "        # print(self.weights)\n",
    "\n",
    "        # assert len(transition_funcs) == len(self.weights)\n",
    "\n",
    "        # function names passed as strings are converted into function object via dictionaries\n",
    "        trans_dict = {\"tanh\": tanh,\n",
    "                      \"logistic\": lambda x: 1 / (1 + exp(-x)),\n",
    "                      \"identity\": lambda x: x}\n",
    "\n",
    "        dtrans_dict = {\"tanh\": lambda x: 1-x**2,\n",
    "                      \"logistic\": lambda x: x*(1-x),\n",
    "                      \"identity\": lambda x: x}\n",
    "                    \n",
    "        self.f_trans = [trans_dict[f] for f in transition_funcs]\n",
    "        self.df_trans = [dtrans_dict[f] for f in transition_funcs]\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    \n",
    "    # one forward of a data through the network and returning the output of each neurone\n",
    "    def forward(self, p_X,p_y):\n",
    "        # list used to cache output of every neuron\n",
    "        cache_f_net = []\n",
    "            \n",
    "        # with this loop, we feed our data forward into all of the layers\n",
    "        layer_input = p_X\n",
    "        for layer_index in range(self.num_layers-1):\n",
    "            net = self._matrix_add(self._matrix_mul(layer_input, self.weights[layer_index]), self.biases[layer_index])\n",
    "            out = [[self.f_trans[layer_index](y) for y in row] for row in net]\n",
    "            cache_f_net.append(out.copy())\n",
    "\n",
    "            layer_input = out\n",
    "        \n",
    "        return cache_f_net\n",
    "\n",
    "    #back propagation of error and finding the weights using gradient decent\n",
    "    def back_prop(self,p_x,p_y,cache):\n",
    "        # this function takes a y,x of teacher and the output of each \n",
    "        # neurone when forwarding this data through the network\n",
    "        global eta\n",
    "        d_y=[[]]\n",
    "        cache=[p_x]+cache\n",
    "        \n",
    "        # Here the result of gradient of each neurone of the outputlayer is calculated\n",
    "        # because we have different f() for each layer, here we have an array of functions\n",
    "        # the index of the array is the number of the layer\n",
    "        for y in cache[-1][0]:\n",
    "            d_y[0].append(self.df_trans[-1](y))\n",
    "        out_delta=self._matrix_dot_mul(self._matrix_sub(p_y,cache[-1]),d_y)\n",
    "        #delta of the last layer is calculated\n",
    "\n",
    "        #here we calculated the Delta_Weights using the delta values of last step\n",
    "        w_def_out=[[outh*delta*eta for outh in cache[-2][0] ] for delta in out_delta[0]]\n",
    "        \n",
    "        # Here we calculated the new weights for last layer using the Delta_weights of last step\n",
    "        for i in range(len(cache[-1][0])):\n",
    "            self.weights[-1][i][0]+=w_def_out[0][i]\n",
    "        w_def_all=[]\n",
    "        w_def_all.append(w_def_out)\n",
    "\n",
    "        # in this loop we calculate the delta for each step and using that we calculate\n",
    "        # the Delta_Weight (difference of the new weight from previus one), for each layer we have an iteration\n",
    "        for i in range(len(cache)-3,-1,-1):\n",
    "            \n",
    "            #out_delta_pre is an array of the delta of previous layer\n",
    "            out_delta_pre=out_delta.copy()\n",
    "            out_delta=[[0 for x in range(len(cache[i+1][0]))]]\n",
    "            w_def=[[0 for x in cache[i+1][0]] for r in cache[i][0]]\n",
    "            #in w_def the difference of the weights for this layer should be stored\n",
    "            \n",
    "            #for eache neurone of this layer we calculate the delta then the Delta_weight and then the new Weight\n",
    "            #start of calculating the delta for the neurone based on the delta of the last layer\n",
    "            for n in range(len(cache[i+1][0])):\n",
    "                sigma_delta=0\n",
    "                for k in range(len(out_delta_pre[0])):\n",
    "                   \n",
    "                    sigma_delta+=out_delta_pre[0][k]*self.weights[i+1][n][k]\n",
    "                    \n",
    "                deltah=sigma_delta*self.df_trans[i](cache[i+1][0][n])\n",
    "                out_delta[0][n]=deltah\n",
    "                \n",
    "               #now we have the delta and we can calculate the difference of weight (Delta_weight)\n",
    "                for j in range(len(cache[i][0])):\n",
    "                   \n",
    "                    w_def[j][n]=eta*deltah*cache[i][0][j]\n",
    "                \n",
    "                w_def_all.insert(0,w_def)\n",
    "\n",
    "            #here we apply the difference of the weight of this layerx\n",
    "            for k in range(len(w_def[0])):\n",
    "                for j in range(len(w_def)):\n",
    "                \n",
    "                    \n",
    "                    self.weights[i][j][k]+=w_def[j][k]\n",
    "         \n",
    "    def train(self, X_train, y_train,X_test,y_test,epoch_count):\n",
    "        for i in range(epoch_count):\n",
    "            print(\"epoch: \",i,\" \\n -----------------------------------\")\n",
    "            for p_X, p_y in zip(X_train, y_train):\n",
    "                # for every instance, data is passed through the network and the outputs are returned\n",
    "                iter_cache=self.forward(p_X,p_y)\n",
    "\n",
    "                # We then, use thos values to calculate losses and update the weights\n",
    "                self.back_prop(p_X,p_y,iter_cache)\n",
    "\n",
    "            # we calculate the cumulative error of the epoch\n",
    "            sum_error=0\n",
    "            for p_X, p_y in zip(X_test, y_test):\n",
    "                res=self.forward(p_X,p_y)\n",
    "                \n",
    "                sum_error+=(p_y[0][0]-res[-1][0][0])**2\n",
    "            avge=sum_error/len(y_test)\n",
    "            print(\"error\",avge)\n",
    "\n",
    "\n",
    "    # We had to implement linear algebra functions in pure python to simplify our code\n",
    "    def _matrix_sub(self,A,B):\n",
    "        return self._matrix_add(A,[[-x for x in row]for row in B])\n",
    "    # implement of adding 2 matrix function\n",
    "    def _matrix_add(self, A, B):\n",
    "        rows_A = len(A)\n",
    "        cols_A = len(A[0])\n",
    "        rows_B = len(B)\n",
    "        cols_B = len(B[0])\n",
    "\n",
    "        #raise error if dimensions are not equal\n",
    "        if rows_A != rows_B or cols_A!=cols_B:\n",
    "            raise \"jam nmisheeeeeeeeeeeee\"\n",
    "        # Create the result matrix\n",
    "        # Dimensions would be rows_A x cols_B\n",
    "        C = [[0 for col in range(cols_B)] for col in range(rows_A)]\n",
    "\n",
    "        for i in range(rows_A):\n",
    "            for j in range(cols_B):\n",
    "                \n",
    "                C[i][j]+=A[i][j]+B[i][j]    \n",
    "        return C\n",
    "    #implementation of crossing two matrixes\n",
    "    def _matrix_mul(self, A, B):\n",
    "        rows_A = len(A)\n",
    "        cols_A = len(A[0])\n",
    "        rows_B = len(B)\n",
    "        cols_B = len(B[0])\n",
    "\n",
    "        #print(rows_A, cols_A, rows_B, cols_B)\n",
    "        if cols_A != rows_B:\n",
    "            raise \"nmisheeeeeeeeeeeee\"\n",
    "        # Create the result matrix\n",
    "        # Dimensions would be rows_A x cols_B\n",
    "        C = [[0 for row in range(cols_B)] for col in range(rows_A)]\n",
    "\n",
    "        for i in range(rows_A):\n",
    "            for j in range(cols_B):\n",
    "                for k in range(cols_A):\n",
    "                    C[i][j]+=A[i][k]*B[k][j]    \n",
    "        return C\n",
    "\n",
    "    #implementation of dot multiply of two matrixes\n",
    "    def _matrix_dot_mul(self,A,B):\n",
    "        rows_A = len(A)\n",
    "        cols_A = len(A[0])\n",
    "        rows_B = len(B)\n",
    "        cols_B = len(B[0])\n",
    "\n",
    "        #print(rows_A, cols_A, rows_B, cols_B)\n",
    "        if rows_A != rows_B or cols_A!=cols_B:\n",
    "            raise \"jam nmisheeeeeeeeeeeee\"\n",
    "        # Create the result matrix\n",
    "        # Dimensions would be rows_A x cols_B\n",
    "        C = [[0 for col in range(cols_B)] for col in range(rows_A)]\n",
    "\n",
    "        for i in range(rows_A):\n",
    "            for j in range(cols_B):\n",
    "                \n",
    "                C[i][j]+=A[i][j]*B[i][j]    \n",
    "        return C\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the variables:\n",
    "hidden_layer_neurons=20\n",
    "epoch_count=40\n",
    "\n",
    "# We first start by reading the data instances from the file\n",
    "input_path = \"PA-A_training_data_01.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X, y = [], []\n",
    "with open(input_path, 'r') as inp:\n",
    "    # the structure of the dataset is parsed from the second line\n",
    "    inp.readline()\n",
    "    in_n, out_n = inp.readline().split()[2:]\n",
    "    in_n = int(in_n[2:])\n",
    "    out_n = int(out_n[2:])\n",
    "\n",
    "    for line in inp:\n",
    "        if line and not line.startswith('#'):\n",
    "            # we again, parse each line by a space to get the list of integer values\n",
    "            vals = [float(v) for v in line.strip('\\n').strip(' ').split(' ') if v]\n",
    "\n",
    "            # we split the values into x and y\n",
    "            X.append([vals[:-out_n]])\n",
    "            y.append([vals[-out_n:]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  \n",
      " -----------------------------------\n",
      "error 1.3656375461256047\n",
      "epoch:  1  \n",
      " -----------------------------------\n",
      "error 1.3234581994736476\n",
      "epoch:  2  \n",
      " -----------------------------------\n",
      "error 1.2724002490229236\n",
      "epoch:  3  \n",
      " -----------------------------------\n",
      "error 1.1957433415923893\n",
      "epoch:  4  \n",
      " -----------------------------------\n",
      "error 1.05682208465237\n",
      "epoch:  5  \n",
      " -----------------------------------\n",
      "error 0.8808665714971391\n",
      "epoch:  6  \n",
      " -----------------------------------\n",
      "error 0.8122823620790282\n",
      "epoch:  7  \n",
      " -----------------------------------\n",
      "error 0.7830777271375964\n",
      "epoch:  8  \n",
      " -----------------------------------\n",
      "error 0.7580589759896614\n",
      "epoch:  9  \n",
      " -----------------------------------\n",
      "error 0.7355674295177644\n",
      "epoch:  10  \n",
      " -----------------------------------\n",
      "error 0.7159644107302162\n",
      "epoch:  11  \n",
      " -----------------------------------\n",
      "error 0.6992966438321245\n",
      "epoch:  12  \n",
      " -----------------------------------\n",
      "error 0.6852519891522824\n",
      "epoch:  13  \n",
      " -----------------------------------\n",
      "error 0.6733904116281827\n",
      "epoch:  14  \n",
      " -----------------------------------\n",
      "error 0.6632907899099247\n",
      "epoch:  15  \n",
      " -----------------------------------\n",
      "error 0.6546017888451141\n",
      "epoch:  16  \n",
      " -----------------------------------\n",
      "error 0.6470458514411923\n",
      "epoch:  17  \n",
      " -----------------------------------\n",
      "error 0.6404078843456867\n",
      "epoch:  18  \n",
      " -----------------------------------\n",
      "error 0.6345216676962253\n",
      "epoch:  19  \n",
      " -----------------------------------\n",
      "error 0.629258087865108\n",
      "epoch:  20  \n",
      " -----------------------------------\n",
      "error 0.6245159518460649\n",
      "epoch:  21  \n",
      " -----------------------------------\n",
      "error 0.6202151028395761\n",
      "epoch:  22  \n",
      " -----------------------------------\n",
      "error 0.6162913373614\n",
      "epoch:  23  \n",
      " -----------------------------------\n",
      "error 0.6126926621694435\n",
      "epoch:  24  \n",
      " -----------------------------------\n",
      "error 0.609376526998584\n",
      "epoch:  25  \n",
      " -----------------------------------\n",
      "error 0.6063077634288032\n",
      "epoch:  26  \n",
      " -----------------------------------\n",
      "error 0.6034570353842181\n",
      "epoch:  27  \n",
      " -----------------------------------\n",
      "error 0.6007996624030927\n",
      "epoch:  28  \n",
      " -----------------------------------\n",
      "error 0.5983147166797865\n",
      "epoch:  29  \n",
      " -----------------------------------\n",
      "error 0.5959843230405779\n",
      "epoch:  30  \n",
      " -----------------------------------\n",
      "error 0.5937931108320078\n",
      "epoch:  31  \n",
      " -----------------------------------\n",
      "error 0.5917277806695915\n",
      "epoch:  32  \n",
      " -----------------------------------\n",
      "error 0.589776758892538\n",
      "epoch:  33  \n",
      " -----------------------------------\n",
      "error 0.5879299196334069\n",
      "epoch:  34  \n",
      " -----------------------------------\n",
      "error 0.5861783594943096\n",
      "epoch:  35  \n",
      " -----------------------------------\n",
      "error 0.5845142135115007\n",
      "epoch:  36  \n",
      " -----------------------------------\n",
      "error 0.5829305037942609\n",
      "epoch:  37  \n",
      " -----------------------------------\n",
      "error 0.5814210142237277\n",
      "epoch:  38  \n",
      " -----------------------------------\n",
      "error 0.5799801860896882\n",
      "epoch:  39  \n",
      " -----------------------------------\n",
      "error 0.5786030306669419\n"
     ]
    }
   ],
   "source": [
    "# Now we can initialize a network and train it\n",
    "nn = Perceptron(layers_structure=(in_n, hidden_layer_neurons, out_n), transition_funcs=[\"tanh\", \"logistic\"], learning_rate=0.001, random_seed=42)\n",
    "nn.train(X, y,X,y,epoch_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
